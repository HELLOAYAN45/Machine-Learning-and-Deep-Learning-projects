{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e7db3f-0b25-4c20-baff-e91eb2d28989",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6fc1946-57d8-4eb5-914d-a3535b227978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e0057-6da4-4a2a-9c7b-5060715eca35",
   "metadata": {},
   "source": [
    "balancing the dataset to 2000 images in every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1eacba7-b628-4930-bd16-34f359cbc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class 'angry': 7215 images, needs 0 more.\n",
      "\n",
      " Class 'disgust': 7215 images, needs 0 more.\n",
      "\n",
      " Class 'fear': 7215 images, needs 0 more.\n",
      "\n",
      " Class 'happy': 7215 images, needs 0 more.\n",
      "\n",
      " Class 'neutral': 7215 images, needs 0 more.\n",
      "\n",
      " Class 'sad': 7215 images, needs 0 more.\n",
      "\n",
      " Class 'surprise': 7215 images, needs 0 more.\n",
      "\n",
      "Dataset is face-balance ready\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = 'C:/Users/Ayan/OneDrive/Desktop/FER_dataset/train' \n",
    "TARGET_COUNT = 7215  # Set to the largest class size or your desired balance count\n",
    "\n",
    "# Gentle augmentations that keep face structure intact\n",
    "augmentor = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "for class_folder in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_folder)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    current_count = len(image_files)\n",
    "    images_needed = TARGET_COUNT - current_count\n",
    "\n",
    "    print(f\"\\n Class '{class_folder}': {current_count} images, needs {images_needed} more.\")\n",
    "\n",
    "    if images_needed <= 0:\n",
    "        continue\n",
    "\n",
    "    augmentations_per_image = math.ceil(images_needed / current_count)\n",
    "\n",
    "    for img_file in tqdm(image_files, desc=f\"Augmenting {class_folder}\"):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "\n",
    "        aug_count = 0\n",
    "        for batch in augmentor.flow(x, batch_size=1):\n",
    "            aug_img = array_to_img(batch[0])\n",
    "            save_name = f\"{os.path.splitext(img_file)[0]}_aug{aug_count}.jpg\"\n",
    "            aug_img.save(os.path.join(class_path, save_name))\n",
    "            aug_count += 1\n",
    "            if aug_count >= augmentations_per_image:\n",
    "                break\n",
    "\n",
    "        if len(os.listdir(class_path)) >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "print(\"\\nDataset is face-balance ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab752f3-cb8a-4221-87fe-4bf8ca018e5f",
   "metadata": {},
   "source": [
    "Part:-1 preprocessing the tarin and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c1e7ce1b-895a-469c-80d9-c4736f49b742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40404 images belonging to 7 classes.\n",
      "Found 10101 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create the data generator - no color_mode here\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Use color_mode in flow_from_directory instead\n",
    "train_data = datagen.flow_from_directory(\n",
    "    'C:/Users/Ayan/OneDrive/Desktop/FER_dataset/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',  # This is correct - color_mode goes here\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    'C:/Users/Ayan/OneDrive/Desktop/FER_dataset/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',  # This is correct - color_mode goes here\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f50b06-9921-4bff-9f92-8be9d20f6b59",
   "metadata": {},
   "source": [
    "Part-2:_ buildinfg the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c25b059d-3629-45ed-aae7-a81810c1987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 63, 63, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 30, 30, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 14, 14, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6517255 (24.86 MB)\n",
      "Trainable params: 6517255 (24.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model with correct input shape for grayscale\n",
    "cnn = tf.keras.models.Sequential([\n",
    "    # First layer with input_shape=(128, 128, 1) for grayscale\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D(pool_size=2, strides=2),\n",
    "    \n",
    "    # Remaining layers\n",
    "    Conv2D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=2, strides=2),\n",
    "    \n",
    "    Conv2D(128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=2, strides=2),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad9201-0358-427e-8979-79e62d3cd476",
   "metadata": {},
   "source": [
    "Part-3:- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94786e0c-efc1-4a0c-9ee5-37f256e7a252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1263/1263 [==============================] - 3971s 3s/step - loss: 1.7468 - accuracy: 0.3081 - val_loss: 1.5711 - val_accuracy: 0.3930\n",
      "Epoch 2/15\n",
      "1263/1263 [==============================] - 2097s 2s/step - loss: 1.4712 - accuracy: 0.4427 - val_loss: 1.4681 - val_accuracy: 0.4433\n",
      "Epoch 3/15\n",
      "1263/1263 [==============================] - 2362s 2s/step - loss: 1.3381 - accuracy: 0.4954 - val_loss: 1.4065 - val_accuracy: 0.4716\n",
      "Epoch 4/15\n",
      " 399/1263 [========>.....................] - ETA: 21:26 - loss: 1.2421 - accuracy: 0.5290"
     ]
    }
   ],
   "source": [
    "model = cnn.fit(\n",
    "    train_data,\n",
    "    epochs=15,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba72c2f-a308-46ce-a830-9c74ddb434ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_compat_env]",
   "language": "python",
   "name": "conda-env-tf_compat_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
