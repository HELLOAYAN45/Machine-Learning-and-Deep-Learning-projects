{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4bb52e-3095-4da7-9a15-3cc160207c34",
   "metadata": {},
   "source": [
    "IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185b2f00-e8ae-45d5-bb0f-deb7659646e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ba4ac-c19b-4b40-bc37-884903786265",
   "metadata": {},
   "source": [
    "PART-1:- DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9ab54b-e639-4cfe-bc77-abbd6e4821ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                   # Normalize pixel values [0, 255] â†’ [0, 1]\n",
    "    rotation_range=20,               # Random rotation\n",
    "    width_shift_range=0.1,           # Horizontal translation\n",
    "    height_shift_range=0.1,          # Vertical translation\n",
    "    shear_range=0.1,                 # Shearing\n",
    "    zoom_range=0.1,                  # Zoom in/out\n",
    "    horizontal_flip=True,            # Flip image horizontally\n",
    "    fill_mode='nearest'              # Fill in pixels after transformation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647d488e-f80a-4b1e-bb4a-aa40cd882f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)#defining validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82697b4c-968a-401b-a9f9-712121ac5304",
   "metadata": {},
   "source": [
    "PREPROCESSING OF TRAINNING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b128551-9f4c-489e-a84f-4384676b1a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'C:/Users/Ayan/OneDrive/Desktop/dataset/training_set', #always put forward slashes rather than backward cause python don support it\n",
    "    target_size=(125, 125),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc03705-3189-42e7-a8f4-844c03ea4035",
   "metadata": {},
   "source": [
    "PREPROCESSING OF VALIDATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30014c81-c485-48b8-bf34-e785f9021d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'C:/Users/Ayan/OneDrive/Desktop/dataset/test_set',\n",
    "    target_size=(125, 125),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd44fac-51de-444a-b0d9-ba960eeed244",
   "metadata": {},
   "source": [
    "PREPROCESSING THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ef4f84-9122-4ecb-913d-9b8f9f27af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'C:/Users/Ayan/OneDrive/Desktop/dataset/test_set',\n",
    "    target_size=(125, 125),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',        # or 'categorical' if multiclass\n",
    "    shuffle=False               # VERY IMPORTANT for predictions & metrics!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191326c6-97be-4db8-958e-f781a49bb4ae",
   "metadata": {},
   "source": [
    "PART-2:- BUILDING THE CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f100e-4157-4873-b5ae-ff5294c3b4db",
   "metadata": {},
   "source": [
    "INITIALIZE THE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8535868-7933-481a-96b6-2e3f02bfbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678bb56a-bdf9-4465-b5a3-90fe8644fd61",
   "metadata": {},
   "source": [
    "CONVOLUTIONAL LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce446223-32ca-4a75-9365-e64823993083",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    strides = 1,\n",
    "    activation = 'relu',\n",
    "    input_shape = [125,125,3]\n",
    "))#use to detect pattern in 2d image for my model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71aa8f-98c5-4aaf-978c-cbaf32a07517",
   "metadata": {},
   "source": [
    "POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b0194e8-d0fd-446c-85e9-c5c8b8050923",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2,strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24f88e-4859-4838-9410-77bfaa9c9f7c",
   "metadata": {},
   "source": [
    "ADDING NEXT CONVOLUTIONAL LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d44a627e-4d0d-4034-975d-0680e1327ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 64,kernel_size = 3,strides = 1,activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2,strides = 2))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 64,kernel_size = 3,strides = 1,activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2,strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61173ae-a68b-4a9a-9220-f8df51baac0b",
   "metadata": {},
   "source": [
    "FLATTENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bfd372d-0b9a-46f4-a46b-87fc0276b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55522c58-c7ea-4a8f-9222-935637282723",
   "metadata": {},
   "source": [
    "FULLY CONNECTED LAYERS OR HIDDEN LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f187ef79-fdbb-40b1-9c6d-8e3a33bcb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128,activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ad601-a0ee-44f3-bd3b-ca03a3a6ec38",
   "metadata": {},
   "source": [
    "OUTPUT LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b9c72b6-c53e-4840-b66f-df9782f62c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac9c72-cdd0-4240-91a9-6d2e9e64b74a",
   "metadata": {},
   "source": [
    "PART-3:- TRAINNING THE CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51b479-2c1e-4602-9a45-49f8b6e7ed9f",
   "metadata": {},
   "source": [
    "COMPILE THE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10ea57ba-4fda-48b0-89b4-f96a53b9040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer ='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b04301e-5446-4079-8f0d-3484e728ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 123, 123, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 61, 61, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 59, 59, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 29, 29, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 27, 27, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10816)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1384576   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1460353 (5.57 MB)\n",
      "Trainable params: 1460353 (5.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab3a63-7bd1-44a9-a044-02d9ed45af80",
   "metadata": {},
   "source": [
    "TRAIN CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e56797b-9b73-4953-8db6-b8a7cd0b3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 360s 1s/step - loss: 0.4519 - accuracy: 0.7846 - val_loss: 0.4236 - val_accuracy: 0.8055\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 351s 1s/step - loss: 0.4188 - accuracy: 0.8060 - val_loss: 0.3959 - val_accuracy: 0.8220\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 360s 1s/step - loss: 0.4245 - accuracy: 0.8076 - val_loss: 0.4076 - val_accuracy: 0.8135\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 360s 1s/step - loss: 0.4032 - accuracy: 0.8126 - val_loss: 0.3900 - val_accuracy: 0.8255\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 367s 1s/step - loss: 0.3964 - accuracy: 0.8188 - val_loss: 0.3743 - val_accuracy: 0.8305\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 368s 1s/step - loss: 0.3878 - accuracy: 0.8249 - val_loss: 0.3623 - val_accuracy: 0.8410\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 368s 1s/step - loss: 0.3759 - accuracy: 0.8280 - val_loss: 0.3466 - val_accuracy: 0.8525\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 366s 1s/step - loss: 0.3660 - accuracy: 0.8371 - val_loss: 0.3627 - val_accuracy: 0.8520\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 366s 1s/step - loss: 0.3586 - accuracy: 0.8345 - val_loss: 0.3670 - val_accuracy: 0.8310\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 367s 1s/step - loss: 0.3497 - accuracy: 0.8457 - val_loss: 0.3463 - val_accuracy: 0.8490\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 366s 1s/step - loss: 0.3515 - accuracy: 0.8461 - val_loss: 0.3314 - val_accuracy: 0.8645\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 367s 1s/step - loss: 0.3399 - accuracy: 0.8476 - val_loss: 0.4450 - val_accuracy: 0.7860\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 371s 1s/step - loss: 0.3273 - accuracy: 0.8547 - val_loss: 0.3946 - val_accuracy: 0.8415\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 368s 1s/step - loss: 0.3166 - accuracy: 0.8656 - val_loss: 0.3147 - val_accuracy: 0.8720\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 366s 1s/step - loss: 0.3179 - accuracy: 0.8585 - val_loss: 0.3234 - val_accuracy: 0.8670\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 364s 1s/step - loss: 0.3041 - accuracy: 0.8648 - val_loss: 0.3244 - val_accuracy: 0.8610\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 362s 1s/step - loss: 0.3028 - accuracy: 0.8686 - val_loss: 0.3085 - val_accuracy: 0.8720\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 362s 1s/step - loss: 0.2998 - accuracy: 0.8690 - val_loss: 0.3358 - val_accuracy: 0.8500\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 363s 1s/step - loss: 0.2824 - accuracy: 0.8783 - val_loss: 0.3123 - val_accuracy: 0.8745\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 365s 1s/step - loss: 0.2893 - accuracy: 0.8751 - val_loss: 0.3094 - val_accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d88a311000>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc478dc0-3a36-489d-93b6-ba881ef107a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 320ms/step\n",
      "{'cats': 0, 'dogs': 1}\n",
      "[[0.93167675]]\n",
      "It's a Dog\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "img_path = 'Downloads/photo-1664822132578-5efe9340d5ee.jpg'\n",
    "img = image.load_img(img_path, target_size=(125, 125))  # Same size as training data\n",
    "\n",
    "# Convert image to array and preprocess\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.0  # Normalize to [0,1] like training data\n",
    "\n",
    "# Make the prediction\n",
    "prediction = cnn.predict(img_array)\n",
    "probability = prediction*100\n",
    "print(train_generator.class_indices) #print the what value the model assigned for cat and dog\n",
    "if prediction[0][0] >= 0.5:\n",
    "    print(\"I am {probability}It's a Dog\")\n",
    "else:\n",
    "    print(\"It's a Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1d3ea-e7f5-40c4-8d9b-504b8ce256e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_compat_env]",
   "language": "python",
   "name": "conda-env-tf_compat_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
